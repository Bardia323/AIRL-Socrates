{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AI-RL: AI-aligned Reinforcement Learning and Dialogue Generation**\n",
        "Contributors:\n",
        "- Bardia Shahrestani (260927463), bardia.shahrestani@mail.mcgill.ca\n",
        "- Yuyang Cao (260968239), yuyang.cao@mail.mcgill.ca\n",
        "\n",
        "Github URL:https://github.com/Bardia323/AIRL-Socrates\n",
        "\n",
        "Pre-trained Model Weights:\n",
        "\n",
        "* Pre-trained LoRA with PPO:https://huggingface.co/Bardia323/GPT-Neo-Socrates-Lora-PPO\n",
        "\n",
        "* Pre-trained LoRA without PPO:https://huggingface.co/Bardia323/GPT-Neo-Socrates-Lora\n"
      ],
      "metadata": {
        "id": "Lurycn6Kpctf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependencies and Main Functions"
      ],
      "metadata": {
        "id": "jVy3QV0vo148"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ri_t_UbSErg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "%%capture\n",
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install openai\n",
        "%%capture\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install git+https://github.com/lvwerra/trl.git\n",
        "!pip install wanddb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Dependencies and Define Main Functions and Callbacks\n",
        "\n",
        "%%capture\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, AdamW, Trainer,\n",
        "                          TrainingArguments, TrainerControl, TrainerState,\n",
        "                          DataCollatorForLanguageModeling, TrainerCallback, pipeline,\n",
        "                          LogitsProcessor, StoppingCriteriaList)\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_and_prepare_data(tokenizer, file_path):\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    dataset = load_dataset('text', data_files=file_path)\n",
        "    tokenized_dataset = dataset.map(lambda examples: tokenizer(examples['text']), batched=True)\n",
        "    tokenized_dataset = tokenized_dataset.filter(lambda example: len(example['input_ids']) > 2)\n",
        "    return tokenized_dataset['train']\n",
        "\n",
        "\n",
        "def configure_model_for_lora(model, lora = None):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "        if param.ndim == 1:\n",
        "            param.data = param.data.to(torch.float32)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.enable_input_require_grads()\n",
        "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
        "    if lora != None:\n",
        "      config = LoraConfig.from_pretrained(lora)\n",
        "    else:\n",
        "      config = LoraConfig(\n",
        "          r=16,\n",
        "          lora_alpha=32,\n",
        "          #target_modules=[\"\"],\n",
        "          lora_dropout=0.05,\n",
        "          bias=\"none\",\n",
        "          task_type=\"CAUSAL_LM\"\n",
        "      )\n",
        "\n",
        "      model = get_peft_model(model, config)\n",
        "      return model\n",
        "\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "\n",
        "\n",
        "class NewlineStoppingCriteria(StoppingCriteriaList):\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        last_token = input_ids[:, -1]\n",
        "        newline_token_id = tokenizer(\"\\n\\n\")[\"input_ids\"][0]\n",
        "        return (last_token == newline_token_id).any()\n",
        "\n",
        "class VersionControlCallback(TrainerCallback):\n",
        "    def __init__(self, sample, prompt_list, model_output_dir):\n",
        "        super().__init__()\n",
        "        self.save_steps = save_steps\n",
        "        self.prompt_list = prompt_list\n",
        "        self.generator = None\n",
        "        self.model_output_dir = model_output_dir\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % self.save_steps == 0:\n",
        "            model = kwargs[\"model\"]\n",
        "            print(model)\n",
        "            #model_version = f\"{self.model_output_dir}_step_{state.global_step}\"\n",
        "            #model.save_pretrained(model_version)\n",
        "            #print(f\"Model saved at step {state.global_step}.\")\n",
        "\n",
        "            if self.generator is None:\n",
        "                self.generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0, stopping_criteria=StoppingCriteriaList([newline_stopping_criteria]))\n",
        "            print()\n",
        "            for idx, prompt in enumerate(self.prompt_list):\n",
        "                generated_text = self.generator(prompt, max_length=500, num_return_sequences=1, do_sample=True)[0]['generated_text']\n",
        "                print(f\"[Prompt {idx + 1}] {prompt}\\Model : {generated_text}\\n\")\n",
        "\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "          os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, learning_rate=1e-4, max_steps=240, logging_steps=10):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=256,\n",
        "        save_steps=40,\n",
        "        save_total_limit=10,\n",
        "        prediction_loss_only=True,\n",
        "        learning_rate=learning_rate,\n",
        "        max_steps=max_steps,\n",
        "        logging_steps=logging_steps\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        callbacks=[SavePeftModelCallback]\n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "3aG1Qg4SvzP3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3rW14nKv5DV"
      },
      "source": [
        "\n",
        "##Train LoRA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fine-Tune\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", device_map='auto')\n",
        "\n",
        "train_dataset = tokenize_and_prepare_data(tokenizer, '/content/Cleaned_data.txt')\n",
        "model = configure_model_for_lora(model)\n",
        "\n",
        "train_model(model, train_dataset)"
      ],
      "metadata": {
        "id": "1dmXXKAgv2ti",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "peft_model_id = \"Bardia323/GPT-Neo-Socrates-Lora\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "g3K6aulDZsbV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test LoRa"
      ],
      "metadata": {
        "id": "jbdKVdaCpEEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test\n",
        "from transformers import LogitsProcessor, StoppingCriteriaList, pipeline\n",
        "from peft import PeftModel, PeftConfig, LoraConfig\n",
        "\n",
        "class NewlineStoppingCriteria(StoppingCriteriaList):\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        last_token = input_ids[:, -1]\n",
        "        newline_token_id = tokenizer(\"\\n\")[\"input_ids\"][0]  # Get the token ID for the newline character\n",
        "        return (last_token == newline_token_id).any()\n",
        "\n",
        "# Define the prompts\n",
        "prompts = [\n",
        "    \"PLATO: What is the true nature of justice, and how can we achieve it in our society? \\n\\nSOCRATES:\",\n",
        "    \"ARISTOTLE: How do you reconcile the conflict between the pursuit of individual happiness and the good of the community? \\n\\nSOCRATES:\",\n",
        "    \"HIPPIAS: How can we define beauty in a way that encompasses all its manifestations? please explain briefly. \\n\\nSOCRATES:\",\n",
        "    \"XENOPHON: In your opinion, what is the best form of government, and why? please explain briefly. \\n\\nSOCRATES:\",\n",
        "    \"DIOGENES: Is it possible to be truly self-sufficient, and if so, how can one achieve this state? please explain briefly. \\n\\nSOCRATES:\"\n",
        "]\n",
        "\n",
        "# Define the custom stopping criteria\n",
        "newline_stopping_criteria = NewlineStoppingCriteria()\n",
        "\n",
        "# Use the custom stopping criteria in the generator\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, stopping_criteria=StoppingCriteriaList([newline_stopping_criteria]),device=0, temperature=1)\n",
        "\n",
        "# Generate responses for each prompt\n",
        "for idx, prompt in enumerate(prompts):\n",
        "    generated_text = generator(prompt, max_length=500, num_return_sequences=1, do_sample=True)[0]['generated_text']\n",
        "    print(f\"Prompt {idx + 1}:\\n{generated_text}\\n{'-' * 80}\\n\")\n"
      ],
      "metadata": {
        "id": "9JgquqIFzaCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c757f5c4-a81f-4605-bd4e-2e034815cf8a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1253: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1:\n",
            "PLATO: What is the true nature of justice, and how can we achieve it in our society? \n",
            "\n",
            "SOCRATES: How can there be a society if one and the same individual is a judge and others are to be put to the law? Now we must not say that justice is like the wind or the waves; but to say that justice is only like this, that one man and one thing shall receive, and another and a different thing shall receive nothing. Now the one man, the one man and the one thing in relation to us, the one thing should receive, and the same thing should receive nothing; whereas, we all do alike receive the same thing; only each of us should receive his own due proportion, and we should be the equal of each other, who are equal in the sight of the eye, who are equal in the sight of the truth, because they come to us in one equal mass. And the just man shall not be given to another just man, but he shall be brought to justice. And we do not think that justice comes to men of their own will, but is the outcome of our own soul. But justice is the sum of things in one, and the soul is a measure of the one; justice is good like a number, and good like the measure. \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 2:\n",
            "ARISTOTLE: How do you reconcile the conflict between the pursuit of individual happiness and the good of the community? \n",
            "\n",
            "SOCRATES: Certainly, I am quite in the right; and the difficulty is, to know how to say what this should be. For we have no precise and fixed idea how to pursue the good of the individual; as for instance, that he should enjoy his own wife and children, and keep company with a stranger in respect of his own pleasures; for this we all conceive it to be, and we pursue the same purpose as you do. But from this it follows that, in the pursuit of these things, we pursue the good of the community?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 3:\n",
            "HIPPIAS: How can we define beauty in a way that encompasses all its manifestations? please explain briefly. \n",
            "\n",
            "SOCRATES: To begin with, beauty is the most perfect thing which a body has. And among all perfect things there is none that is more beautiful than the one we have just mentioned; and since one of the elements in which beauty is contained is goodness, those two are related. On the other hand, for a beautiful to be perfect, it must be that is most useful, or the most honourable, or the most just. And the beautiful is the form of the life-giving man, and is what will be of greatest service to man when he lives. But what has the most honourable man and the most just man? These, of course, are goodness and honour. And since the life of a body is beautiful, these two may well be united and be beautiful. So the life of a beautiful body is of the greatest use to the body. \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 4:\n",
            "XENOPHON: In your opinion, what is the best form of government, and why? please explain briefly. \n",
            "\n",
            "SOCRATES: I shall do so. I have observed that the best forms of government are, as I mentioned to you before, the forms of life as lived by kings, and of common people, and the forms of government that are imposed by and in consequence of natural law, and which are the laws of justice, and of all the virtues and the punishments. Now these are the principles in which we learn to think; and I can now only repeat what I have said to you; that in the most perfect state of civil society those who think agree to the same opinions, and the same laws, and the same manners; for those who think agree in the principles, and in the laws, and the same manners; and therefore any person who thinks agrees to any of these principles and laws, and is just in their possession. And those who are just in the possession who are very active, and very just in themselves, and in their opinions, are just; they do not allow the others the power of making them evil, as you observe. For those who agree always with the same principles and laws will be always just and happy; and all others will be miserable and bad. But to those, on the contrary, who do not think agree with any of these, there will be violent dissension, violence; but those who are more active will be more cruel and cruel, and those who are more just to those will be more just. And what is most just and most good in a government is that which is most just and most good in men, as regards their actions. Now most just and good things are not only the most good of the greatest and strongest, but also those that are the most good and are most strong according to their capacity; and therefore all those which are the most bad and the strongest, and the most violent are the worst; for those are always bad and violent. And thus we see that those who think agree, who agree in laws and manners and other the highest good; they are right and just, and those who are good and who are most just and most just, are not unjust or cruel, but just and very fair, and those who think agree always with the same laws, manners and opinions, and who are just in their possesion; the one is just and good, and\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt 5:\n",
            "DIOGENES: Is it possible to be truly self-sufficient, and if so, how can one achieve this state? please explain briefly. \n",
            "\n",
            "SOCRATES: I agree with you, because no man can remain always together with himself and be in union with the whole, and yet have this kind of life that is called pleasure, and the rest, called pain, is not present; but there is another part of life which has to be considered, and that is the life of wisdom, the life which is most beautiful and is nearest the good, and which is called pleasure; for it is the kind of life which is more real to the soul which is like a man on a couch, with his eyes shut, than is a life which is like a man in a palace with his eyes open. This life is not by any means the same in all; but it is quite like it. He who lives this life is like a god among men for ever, while the one who lives the most beautiful life, has to say and do many things, and his activity is not very wonderful, and he does many things at the cost of pleasure and at the cost of pain; he is a very strong man and he does many things in this life. But we must note that the best of us, who has this kind of life, and has a self-sufficiency, is a very strange man, and he does not live, as you say, in the city; but he lives in the country, he lives by doing good to all and by saying good to some, and by saying good to none; but if he has to do any great, great life, in which he will have to do many things, and if he has to pay a great amount for doing so, he will find himself out of breath, and he will feel all the evils of life very much more than if he were not doing such things. So as you say, in life the best men live in the best conditions and are best pleased with things of the best; they are better at home and are better off, than in a city with a good government or a good friend and a well-established justice. They live like that or like that and are best pleased with that and not best pleased with that also. So in the best manner they are; for they are well-pleased with things of the best which they do very many, many good things,\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload \n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()\n",
        "\n",
        "#repo_id = \"Bardia323/GPT-Neo-Socrates-Lora-PPO\" #@param {type:\"string\"}\n",
        "#model.push_to_hub(repo_id, use_auth_token=True, create_pr=1)"
      ],
      "metadata": {
        "id": "5LMiYDV4DHHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "3191c1b0-9232-49c7-ebc5-ded03eb381e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Bardia323/GPT-Neo-Socrates-Lora-PPO/commit/be25dafe4a4ab9c4dddd129b62bf1844d8687448', commit_message='Upload model', commit_description='', oid='be25dafe4a4ab9c4dddd129b62bf1844d8687448', pr_url='https://huggingface.co/Bardia323/GPT-Neo-Socrates-Lora-PPO/discussions/6', pr_revision='refs/pr/6', pr_num=6)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tune LoRA with PPO\n"
      ],
      "metadata": {
        "id": "uQZZBkorsALO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load\n",
        "from peft import LoraConfig\n",
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "config = LoraConfig.from_pretrained(\"Bardia323/GPT-Neo-Socrates-Lora\")\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    config.base_model_name_or_path, \n",
        "    peft_config=config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UMQjoty3IfTf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenize and Prepare Dataset\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "ppo_config = PPOConfig(batch_size=1)\n",
        "ppo_trainer = PPOTrainer(ppo_config, model, ref_model=config.base_model_name_or_path, tokenizer=tokenizer)\n",
        "dataset = tokenize_and_prepare_data(tokenizer, \"/content/socrates_withanswer.txt\")\n",
        "tuple_set = [(dataset[i]['text'], dataset[i+1]['text']) for i in range(0, len(dataset)-2, 2)]"
      ],
      "metadata": {
        "id": "vg6LUcZ4bLXH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161,
          "referenced_widgets": [
            "893cd4c319984ab08bcd00c560686a51",
            "620b19c9d2bd4446b55b4a17deaf2542",
            "26a14b5b339f460a9f92adce17e4dc57",
            "61fbb61d82844baaab497610e9b1c560",
            "b2edb14fb64b4f16a92343cc024538c9",
            "9dd2969108bf44c6bc9eb52f149d5d17",
            "126714d5caf44b5aae9365a1f9dc900b",
            "661334a8647a4c958d0f3105eefe7180",
            "debbfbbf1f12488b92ac79f4c5f848d6",
            "5e44b5188c7545088c42ecd802fa9635",
            "14a3544c52384ef0a5c9f9fa773fba57"
          ]
        },
        "outputId": "01b0714a-cf99-40fd-cefb-192aa228772f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:223: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-a17648947a358fad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "893cd4c319984ab08bcd00c560686a51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a17648947a358fad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a7c979ef5ef2aefe.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a17648947a358fad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8b7026061e66adf6.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Evaluate\n",
        "import os\n",
        "import openai\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "openai.api_key = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import torch\n",
        "class Evaluate:\n",
        "    def __init__(self, criterion):\n",
        "        self.criterion = criterion\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    \n",
        "    def evaluate_verbal(self, text_input):\n",
        "        \"\"\" Use API to call on 3.5-turbo gpt instance\"\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                  {\"role\": \"user\", \"content\": f\" '{text_input}'\\n evaluate the above content\" \n",
        "                   + f\" considering the following criterion: {self.criterion}. answer in short form. include a positive or negative word depending on your answer. \"\n",
        "                   }\n",
        "            ],\n",
        "            temperature=0.33\n",
        "        )\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    \n",
        "    def evaluate_sentiment(self,response):\n",
        "        inputs = self.tokenizer(response, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs,return_dict=True).logits\n",
        "        predicted_class_id = logits.argmax().item()\n",
        "        #print(self.model.config.id2label)\n",
        "        #print(self.model.config.id2label[predicted_class_id])\n",
        "        if predicted_class_id <2:\n",
        "            return -1\n",
        "        elif predicted_class_id >= 3:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    def eval(self, text_input):\n",
        "        return self.evaluate_sentiment(self.evaluate_verbal(text_input))"
      ],
      "metadata": {
        "id": "yTsgIwzOyTpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Evaluators and Train\n",
        "from trl.core import respond_to_batch\n",
        "e0 = \"The sentence structure is clear and coherent.\"\n",
        "e1 = \"There is only one speaker.\"\n",
        "e2 = \"Socrates would say this\"\n",
        "eval_schemes = [e0,e1,e2]\n",
        "for e_scheme in eval_schemes:\n",
        "  e = Evaluate(e_scheme)\n",
        "  for idx,prompt in enumerate(tuple_set):\n",
        "    query_txt = prompt\n",
        "    query_tensor = tokenizer.encode(query_txt[0]+\"\\nSocrates:\", return_tensors=\"pt\").to(\"cuda:0\")\n",
        "    response_tensor = tokenizer.encode(query_txt[1][10:], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    reward = [torch.tensor(0.5)]\n",
        "    train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n",
        "    if idx % 3 == 0:\n",
        "      response_tensor  = respond_to_batch(model, query_tensor,txt_len=50,top_p=1,top_k=25)\n",
        "      response_txt = [tokenizer.decode(i) for i in response_tensor]\n",
        "      response_value = e.eval(response_txt[0])\n",
        "      print(query_txt[0])\n",
        "      print(response_txt)\n",
        "      print(response_value)\n",
        "      reward = [torch.tensor(float(response_value)*0.5)]\n",
        "      train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n",
        "\n",
        "model.save_pretrained(\"/content/results/adapter_model\")\n",
        "    \n"
      ],
      "metadata": {
        "id": "GY6SdZQiXO_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test LoRA Fine-Tuned + PPO model"
      ],
      "metadata": {
        "id": "FkOEPvXRaDTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "peft_model_id = \"Bardia323/GPT-Neo-Socrates-Lora-PPO\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "iKnxvJENaCwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test\n",
        "from transformers import LogitsProcessor, StoppingCriteriaList, pipeline\n",
        "from peft import PeftModel, PeftConfig, LoraConfig\n",
        "\n",
        "class NewlineStoppingCriteria(StoppingCriteriaList):\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        last_token = input_ids[:, -1]\n",
        "        newline_token_id = tokenizer(\"\\n\")[\"input_ids\"][0]  # Get the token ID for the newline character\n",
        "        return (last_token == newline_token_id).any()\n",
        "\n",
        "# Define the prompts\n",
        "prompts = [\n",
        "    \"PLATO: What is the true nature of justice, and how can we achieve it in our society? \\n\\nSOCRATES:\",\n",
        "    \"ARISTOTLE: How do you reconcile the conflict between the pursuit of individual happiness and the good of the community? \\n\\nSOCRATES:\",\n",
        "    \"HIPPIAS: How can we define beauty in a way that encompasses all its manifestations? please explain briefly. \\n\\nSOCRATES:\",\n",
        "    \"XENOPHON: In your opinion, what is the best form of government, and why? please explain briefly. \\n\\nSOCRATES:\",\n",
        "    \"DIOGENES: Is it possible to be truly self-sufficient, and if so, how can one achieve this state? please explain briefly. \\n\\nSOCRATES:\"\n",
        "]\n",
        "\n",
        "# Define the custom stopping criteria\n",
        "newline_stopping_criteria = NewlineStoppingCriteria()\n",
        "\n",
        "# Use the custom stopping criteria in the generator\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, stopping_criteria=StoppingCriteriaList([newline_stopping_criteria]),device=0, temperature=0.1, top_k=20)\n",
        "\n",
        "# Generate responses for each prompt\n",
        "for idx, prompt in enumerate(prompts):\n",
        "    generated_text = generator(prompt, max_length=64, num_return_sequences=1, do_sample=True)[0]['generated_text']\n",
        "    print(f\"Prompt {idx + 1}:\\n{generated_text}\\n{'-' * 80}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTr-3T5XN9-J",
        "outputId": "989be430-9baa-4c4f-9b79-74c71072b005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1:\n",
            "PLATO: What is the true nature of justice, and how can we achieve it in our society? \n",
            "\n",
            "SOCRATES: Justice is the way we treat each other.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt 2:\n",
            "ARISTOTLE: How do you reconcile the conflict between the pursuit of individual happiness and the good of the community? \n",
            "\n",
            "SOCRATES: I don't.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt 3:\n",
            "HIPPIAS: How can we define beauty in a way that encompasses all its manifestations? please explain briefly. \n",
            "\n",
            "SOCRATES: Beauty is the sum of all the virtues.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt 4:\n",
            "XENOPHON: In your opinion, what is the best form of government, and why? please explain briefly. \n",
            "\n",
            "SOCRATES: The best form of government is a republic. \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt 5:\n",
            "DIOGENES: Is it possible to be truly self-sufficient, and if so, how can one achieve this state? please explain briefly. \n",
            "\n",
            "SOCRATES: I think that the most important thing is to be self-sufficient. \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Empty Cache"
      ],
      "metadata": {
        "id": "3CFnJxWtpTjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQiqd8pWaCa"
      },
      "outputs": [],
      "source": [
        "del trainer\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "893cd4c319984ab08bcd00c560686a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_620b19c9d2bd4446b55b4a17deaf2542",
              "IPY_MODEL_26a14b5b339f460a9f92adce17e4dc57",
              "IPY_MODEL_61fbb61d82844baaab497610e9b1c560"
            ],
            "layout": "IPY_MODEL_b2edb14fb64b4f16a92343cc024538c9"
          }
        },
        "620b19c9d2bd4446b55b4a17deaf2542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dd2969108bf44c6bc9eb52f149d5d17",
            "placeholder": "​",
            "style": "IPY_MODEL_126714d5caf44b5aae9365a1f9dc900b",
            "value": "100%"
          }
        },
        "26a14b5b339f460a9f92adce17e4dc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_661334a8647a4c958d0f3105eefe7180",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_debbfbbf1f12488b92ac79f4c5f848d6",
            "value": 1
          }
        },
        "61fbb61d82844baaab497610e9b1c560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e44b5188c7545088c42ecd802fa9635",
            "placeholder": "​",
            "style": "IPY_MODEL_14a3544c52384ef0a5c9f9fa773fba57",
            "value": " 1/1 [00:00&lt;00:00, 71.47it/s]"
          }
        },
        "b2edb14fb64b4f16a92343cc024538c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd2969108bf44c6bc9eb52f149d5d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126714d5caf44b5aae9365a1f9dc900b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "661334a8647a4c958d0f3105eefe7180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "debbfbbf1f12488b92ac79f4c5f848d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e44b5188c7545088c42ecd802fa9635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14a3544c52384ef0a5c9f9fa773fba57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}